import os
import datetime as dt
import requests
import psycopg2
from psycopg2.extras import execute_values
from dotenv import load_dotenv

load_dotenv()

# ========== iiko ==========
IIKO_BASE_URL = (os.getenv("IIKO_BASE_URL", "") or "").strip().rstrip("/")
IIKO_LOGIN = os.getenv("IIKO_LOGIN")
IIKO_PASSWORD = os.getenv("IIKO_PASSWORD")

DEPARTMENTS = ["–ê–≤–∏–∞–≥–æ—Ä–æ–¥–æ–∫", "–î–æ–º–æ–¥–µ–¥–æ–≤–æ"]
PRODUCT_NUM_FILTER = [
    "0722",
    "45700042712",
    "45700042362",
    "45700041658",
    "45700042237",
    "45700042841",
    "45700042013",
    "25551",
    "45700042089",
    "0603",
    "45700042183",
    "0607",
    "45700041955",
    "06163",
    "4570004177",
    "45700041757",
    "0617",
    "45700041956",
    "45700042665",
    "45700041625",
    "45700041762",
    "2313231233122312335",
    "00001",
    "00002",
    "00003",
]  # –µ—Å–ª–∏ –Ω–µ –Ω—É–∂–µ–Ω ‚Äî —Å–¥–µ–ª–∞–π []


# ---------- helpers for iiko urls ----------
def _join(base: str, path: str) -> str:
    return base.rstrip("/") + "/" + path.lstrip("/")


def iiko_api_url(path: str, use_resto: bool) -> str:
    """
    use_resto=True  -> BASE/resto/<path>
    use_resto=False -> BASE/<path>
    """
    base = IIKO_BASE_URL
    if use_resto:
        if base.endswith("/resto"):
            return _join(base, path)
        return _join(base, "/resto/" + path.lstrip("/"))
    return _join(base, path)


def request_with_resto_fallback(method: str, path: str, **kwargs):
    """
    –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º BASE/<path>, –µ—Å–ª–∏ 404 ‚Äî –ø—Ä–æ–±—É–µ–º BASE/resto/<path>.
    """
    if not IIKO_BASE_URL:
        raise RuntimeError("IIKO_BASE_URL is not set")

    url1 = iiko_api_url(path, use_resto=False)
    resp = requests.request(method, url1, **kwargs)

    if resp.status_code == 404:
        url2 = iiko_api_url(path, use_resto=True)
        resp2 = requests.request(method, url2, **kwargs)
        return resp2, url2

    return resp, url1


# ========== Postgres (Neon) ==========
def get_pg_connection():
    return psycopg2.connect(
        host=os.getenv("PG_HOST"),
        port=os.getenv("PG_PORT"),
        dbname=os.getenv("PG_DB"),
        user=os.getenv("PG_USER"),
        password=os.getenv("PG_PASSWORD"),
        sslmode=os.getenv("PG_SSLMODE", "require"),
    )


# ========== Period ==========
def get_period():
    date_from_str = os.getenv("DATE_FROM")
    date_to_str = os.getenv("DATE_TO")

    if date_from_str and date_to_str:
        date_from = dt.date.fromisoformat(date_from_str)
        date_to = dt.date.fromisoformat(date_to_str)
        print(f"üìÖ –ü–µ—Ä–∏–æ–¥ –∏–∑ ENV: {date_from} ‚Äî {date_to}")
        return date_from, date_to

    LOOKBACK_DAYS = int(os.getenv("LOOKBACK_DAYS", "7"))

    today = dt.date.today()
    date_from = today - dt.timedelta(days=LOOKBACK_DAYS)
    date_to = today

    print(f"üìÖ –ü–µ—Ä–∏–æ–¥ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–ø–æ—Å–ª–µ–¥–Ω–∏–µ {LOOKBACK_DAYS} –¥–Ω–µ–π): {date_from} ‚Äî {date_to}")
    return date_from, date_to


# ========== Auth ==========
def get_token() -> str:
    if not IIKO_LOGIN or not IIKO_PASSWORD:
        raise RuntimeError("IIKO_LOGIN / IIKO_PASSWORD is not set")

    resp, used_url = request_with_resto_fallback(
        "GET",
        "/api/auth",
        params={"login": IIKO_LOGIN, "pass": IIKO_PASSWORD},
        timeout=30,
    )
    print(f"üåê AUTH URL: {used_url}")
    resp.raise_for_status()

    token = resp.text.strip()
    print(f"üîë –¢–æ–∫–µ–Ω –ø–æ–ª—É—á–µ–Ω: {token[:6]}...")
    return token


def logout(token: str):
    if not token:
        return
    try:
        resp, used_url = request_with_resto_fallback(
            "POST",
            "/api/logout",
            params={"key": token},
            timeout=10,
        )
        print(f"üåê LOGOUT URL: {used_url} ({resp.status_code})")
    except Exception as e:
        print("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ logout:", e)


# ========== iiko OLAP ==========
def fetch_stock_tx(token: str, date_from: dt.date, date_to: dt.date):
    print("üì¶ –ó–∞–≥—Ä—É–∂–∞–µ–º OLAP '–û—Ç—á–µ—Ç –ø–æ –ø—Ä–æ–≤–æ–¥–∫–∞–º' –∏–∑ iiko...")

    filters = {
        "DateTime.OperDayFilter": {
            "filterType": "DateRange",
            "periodType": "CUSTOM",
            "from": date_from.strftime("%Y-%m-%d"),
            "to": date_to.strftime("%Y-%m-%d"),
            "includeLow": True,
            "includeHigh": False,  # –í–ê–ñ–ù–û: oper_day < date_to
        },
        "Department": {
            "filterType": "IncludeValues",
            "values": DEPARTMENTS,
        },
    }

    if PRODUCT_NUM_FILTER:
        filters["Product.Num"] = {
            "filterType": "IncludeValues",
            "values": PRODUCT_NUM_FILTER,
        }

    body = {
        "reportType": "TRANSACTIONS",
        "groupByRowFields": [
            "DateTime.DateTyped",
            "Product.Num",
            "Product.Name",
            "Department",
            "Product.Type",
            "Product.MeasureUnit",
            "Document",
            "TransactionType",
        ],
        "aggregateFields": ["Amount.StoreInOutTyped"],
        "filters": filters,
    }

    resp, used_url = request_with_resto_fallback(
        "POST",
        "/api/v2/reports/olap",
        params={"key": token},
        json=body,
        timeout=90,
    )
    print(f"üåê OLAP URL: {used_url}")
    resp.raise_for_status()

    data = resp.json()
    rows = []
    for r in data.get("data", []):
        oper_raw = r.get("DateTime.DateTyped")
        oper_day = oper_raw[:10] if isinstance(oper_raw, str) else oper_raw

        rows.append(
            {
                "department": r.get("Department"),
                "oper_day": oper_day,
                "product_num": r.get("Product.Num"),
                "product_name": r.get("Product.Name"),
                "product_type": r.get("Product.Type"),
                "measure_unit": r.get("Product.MeasureUnit"),
                "document": r.get("Document"),
                "transaction_type": r.get("TransactionType"),
                "turnover": float(r.get("Amount.StoreInOutTyped") or 0),
            }
        )

    print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ —Å—Ç—Ä–æ–∫ –∏–∑ iiko: {len(rows)}")
    print("üîé –ü–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫ –∏–∑ iiko:")
    for i, x in enumerate(rows[:10], start=1):
        print(f"{i:02d}. {x}")

    return rows


# ========== DB schema helpers ==========
def get_table_columns(conn, table_name: str, schema: str = "public") -> set[str]:
    q = """
        SELECT column_name
        FROM information_schema.columns
        WHERE table_schema = %s AND table_name = %s;
    """
    with conn.cursor() as cur:
        cur.execute(q, (schema, table_name))
        return {r[0] for r in cur.fetchall()}


def table_exists(conn, table_name: str, schema: str = "public") -> bool:
    q = """
        SELECT EXISTS (
            SELECT 1
            FROM information_schema.tables
            WHERE table_schema = %s AND table_name = %s
        );
    """
    with conn.cursor() as cur:
        cur.execute(q, (schema, table_name))
        return bool(cur.fetchone()[0])


def pick_turnover_column(cols: set[str]) -> str:
    for cand in ("turnover", "store_in_out", "amount_store_in_out", "amount"):
        if cand in cols:
            return cand
    raise RuntimeError(
        "–ù–µ –Ω–∞—à—ë–ª –∫–æ–ª–æ–Ω–∫—É –ø–æ–¥ –æ–±–æ—Ä–æ—Ç. –û–∂–∏–¥–∞–ª –æ–¥–Ω—É –∏–∑: turnover / store_in_out / amount_store_in_out / amount"
    )


def aggregate_rows(rows: list[dict], with_document: bool) -> list[dict]:
    """
    –î–ª—è with_document=True:
      - —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å: (department, product_num, document, transaction_type)
      - oper_day = MAX
      - turnover = SUM
    –î–ª—è without document ‚Äî —Å—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞
    """

    agg = {}

    for r in rows:
        if with_document and r.get("document") not in (None, ""):
            key = (
                r.get("department"),
                r.get("product_num"),
                r.get("document"),
                r.get("transaction_type"),
            )
        else:
            key = (
                r.get("department"),
                r.get("oper_day"),
                r.get("product_num"),
                r.get("transaction_type"),
            )

        if key not in agg:
            agg[key] = dict(r)
        else:
            # —Å—É–º–º–∏—Ä—É–µ–º –æ–±–æ—Ä–æ—Ç
            agg[key]["turnover"] = float(agg[key].get("turnover") or 0) + float(r.get("turnover") or 0)

            # –¥–ª—è document ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º oper_day –Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π
            if with_document and r.get("document") not in (None, ""):
                agg[key]["oper_day"] = max(agg[key]["oper_day"], r.get("oper_day"))

    out = list(agg.values())
    print(f"üìå –ü–æ—Å–ª–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ ({'—Å document' if with_document else '–±–µ–∑ document'}): {len(out)} —Å—Ç—Ä–æ–∫")
    return out

# ========== Upsert ==========
def upsert_stock_tx(conn, rows: list[dict]):
    if not rows:
        print("‚ö†Ô∏è –ù–µ—Ç —Å—Ç—Ä–æ–∫ –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ –ë–î")
        return 0

    cols = get_table_columns(conn, "stock_tx_iiko", "public")
    has_document = "document" in cols
    turnover_col = pick_turnover_column(cols)

    if not has_document:
        raise RuntimeError("–í stock_tx_iiko –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ document ‚Äî —Ç–µ–∫—É—â–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞")

    rows = aggregate_rows(rows, with_document=True)

    # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ—Ç–æ–∫–∏: —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º –∏ –±–µ–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    rows_with_doc = [r for r in rows if r.get("document") not in (None, "")]
    rows_no_doc = [r for r in rows if r.get("document") in (None, "")]

    insert_cols = [
        "department",
        "oper_day",
        "product_num",
        "product_name",
        "product_type",
        "measure_unit",
        "document",
        "transaction_type",
        turnover_col,
        "updated_at",
    ]
    cols_sql = ", ".join(insert_cols)

    placeholders = ["%s"] * (len(insert_cols) - 1)
    template = "(" + ",".join(placeholders) + ",now())"

    total_written = 0

    # ---------- 1) UPSERT –¥–ª—è —Å—Ç—Ä–æ–∫ –° –¥–æ–∫—É–º–µ–Ω—Ç–æ–º ----------
    # –í–ê–ñ–ù–û:
    # - –∫–æ–Ω—Ñ–ª–∏–∫—Ç —Ç–∞—Ä–≥–µ—Ç: (department, product_num, document, transaction_type) WHERE document IS NOT NULL
    # - –ø—Ä–∏ –∞–ø–¥–µ–π—Ç–µ –æ–±–Ω–æ–≤–ª—è–µ–º oper_day (–¥–æ–∫—É–º–µ–Ω—Ç –º–æ–≥ "–ø–µ—Ä–µ–µ—Ö–∞—Ç—å" –Ω–∞ –¥—Ä—É–≥—É—é –¥–∞—Ç—É)
    if rows_with_doc:
        values = []
        for r in rows_with_doc:
            values.append(
                (
                    r.get("department"),
                    r.get("oper_day"),
                    r.get("product_num"),
                    r.get("product_name"),
                    r.get("product_type"),
                    r.get("measure_unit"),
                    r.get("document"),
                    r.get("transaction_type"),
                    float(r.get("turnover") or 0),
                )
            )

        sql_with_doc = f"""
            INSERT INTO stock_tx_iiko ({cols_sql})
            VALUES %s
            ON CONFLICT (department, product_num, document, transaction_type)
            WHERE document IS NOT NULL
            DO UPDATE SET
                oper_day = EXCLUDED.oper_day,
                product_name = EXCLUDED.product_name,
                product_type = EXCLUDED.product_type,
                measure_unit = EXCLUDED.measure_unit,
                {turnover_col} = EXCLUDED.{turnover_col},
                updated_at = now();
        """

        with conn.cursor() as cur:
            execute_values(cur, sql_with_doc, values, template=template, page_size=500)

        conn.commit()
        total_written += len(values)
        print(f"‚úÖ upsert (by doc key) –∑–∞–ø–∏—Å–∞–Ω–æ: {len(values)}")

    # ---------- 2) UPSERT –¥–ª—è —Å—Ç—Ä–æ–∫ –ë–ï–ó –¥–æ–∫—É–º–µ–Ω—Ç–∞ ----------
    # –¢—É—Ç –æ—Å—Ç–∞–≤–ª—è–µ–º ‚Äú—Å—Ç–∞—Ä—É—é‚Äù –ø—Ä–∏–≤—è–∑–∫—É –∫ oper_day, –ø–æ—Ç–æ–º—É —á—Ç–æ –¥–æ–∫—É–º–µ–Ω—Ç = NULL (—É–Ω–∏–∫–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—á–µ–º)
    if rows_no_doc:
        values = []
        for r in rows_no_doc:
            # document –∫–ª–∞–¥—ë–º –∫–∞–∫ None
            values.append(
                (
                    r.get("department"),
                    r.get("oper_day"),
                    r.get("product_num"),
                    r.get("product_name"),
                    r.get("product_type"),
                    r.get("measure_unit"),
                    None,
                    r.get("transaction_type"),
                    float(r.get("turnover") or 0),
                )
            )

        sql_no_doc = f"""
            INSERT INTO stock_tx_iiko ({cols_sql})
            VALUES %s
            ON CONFLICT (department, oper_day, product_num, document, transaction_type)
            DO UPDATE SET
                product_name = EXCLUDED.product_name,
                product_type = EXCLUDED.product_type,
                measure_unit = EXCLUDED.measure_unit,
                {turnover_col} = EXCLUDED.{turnover_col},
                updated_at = now();
        """

        with conn.cursor() as cur:
            execute_values(cur, sql_no_doc, values, template=template, page_size=500)

        conn.commit()
        total_written += len(values)
        print(f"‚úÖ upsert (by day key, doc=NULL) –∑–∞–ø–∏—Å–∞–Ω–æ: {len(values)}")

    return total_written


def print_db_sample(conn, date_from: dt.date, date_to: dt.date):
    cols = get_table_columns(conn, "stock_tx_iiko", "public")
    has_document = "document" in cols
    turnover_col = pick_turnover_column(cols)

    print("üóÑÔ∏è –ü–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫ –∏–∑ –ë–î –∑–∞ –ø–µ—Ä–∏–æ–¥:")

    select_cols = ["department", "oper_day", "product_num"]
    if has_document:
        select_cols.append("document")
    select_cols += ["transaction_type", turnover_col]

    q = f"""
        SELECT {", ".join(select_cols)}
        FROM stock_tx_iiko
        WHERE oper_day >= %s AND oper_day < %s
        ORDER BY oper_day, department, product_num
        LIMIT 10;
    """
    with conn.cursor() as cur:
        cur.execute(q, (date_from, date_to))
        rows = cur.fetchall()
        for i, r in enumerate(rows, start=1):
            print(f"{i:02d}. {r}")


# ========== Refresh DataLens vitrine ==========
def refresh_datalens_tail(conn, date_from: dt.date, date_to: dt.date):
    """
    –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–∏—Ç—Ä–∏–Ω—É batch_daily_lifecycle —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ stock_tx_iiko.

    –í OLAP —É –Ω–∞—Å oper_day –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ: date_from <= oper_day < date_to
    –î–ª—è snapshot_day —ç—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç: date_from .. (date_to - 1) –≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ.
    """
    snapshot_from = date_from
    snapshot_to = date_to - dt.timedelta(days=1)

    if snapshot_to < snapshot_from:
        print("‚ö†Ô∏è –ü–µ—Ä–∏–æ–¥ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è –ø–µ—Ä–µ—Å—á—ë—Ç–∞ –≤–∏—Ç—Ä–∏–Ω—ã, –ø—Ä–æ–ø—É—Å–∫–∞—é")
        return

    print(f"üßÆ –ü–µ—Ä–µ—Å—á—ë—Ç –≤–∏—Ç—Ä–∏–Ω—ã batch_daily_lifecycle: {snapshot_from} ‚Äî {snapshot_to}")

    with conn.cursor() as cur:
        # 1) –ü—Ä–æ–±—É–µ–º –ø–µ—Ä–µ—Å—á—ë—Ç –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω—É, –µ—Å–ª–∏ –ø—Ä–æ—Ü–µ–¥—É—Ä–∞ –µ—Å—Ç—å
        try:
            cur.execute("CALL public.refresh_batch_daily_lifecycle_range(%s, %s);", (snapshot_from, snapshot_to))
            conn.commit()
            print("‚úÖ –í–∏—Ç—Ä–∏–Ω–∞ –ø–µ—Ä–µ—Å—á–∏—Ç–∞–Ω–∞ —á–µ—Ä–µ–∑ refresh_batch_daily_lifecycle_range")
            return
        except Exception as e:
            conn.rollback()
            print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–∑–≤–∞—Ç—å refresh_batch_daily_lifecycle_range, –ø—Ä–æ–±—É—é fallback:", str(e)[:300])

        # 2) Fallback: –ø–µ—Ä–µ—Å—á—ë—Ç ‚Äú—Ö–≤–æ—Å—Ç–∞‚Äù –ø–æ p_days
        p_days = (snapshot_to - snapshot_from).days + 1
        cur.execute("CALL public.refresh_batch_daily_lifecycle(%s);", (p_days,))
        conn.commit()
        print(f"‚úÖ –í–∏—Ç—Ä–∏–Ω–∞ –ø–µ—Ä–µ—Å—á–∏—Ç–∞–Ω–∞ —á–µ—Ä–µ–∑ refresh_batch_daily_lifecycle(p_days => {p_days})")


# ========== Refresh anchor diffs (Plan/Fact discrepancies) ==========
def refresh_anchor_discrepancies(conn):
    """
    –ò—Ç–æ–≥–æ–≤–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ:
    - –ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º –¢–û–õ–¨–ö–û public.batch_anchor_diff (–¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –ø–∞—Ä—Ç–∏—è–º)
    - —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤ –±–µ—Ä—ë–º –∏–∑ –∏–Ω–≤–µ–Ω—Ç—ã (batch_manual_anchor),
      –∞ –ø–∞—Ä—Ç–∏–∏ (production_day) –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –í–°–ï –∏–∑ batch_daily_lifecycle –ø–æ —ç—Ç–∏–º —Ç–æ–≤–∞—Ä–∞–º –Ω–∞ anchor_day
    - –µ—Å–ª–∏ —è–∫–æ—Ä–µ–π –Ω–µ—Ç -> batch_anchor_diff –±—É–¥–µ—Ç –ø—É—Å—Ç–æ–π
    """

    # —Ç–∞–±–ª–∏—Ü—ã-–∏—Å—Ç–æ—á–Ω–∏–∫–∏
    if not table_exists(conn, "batch_manual_anchor", "public"):
        print("‚ÑπÔ∏è batch_manual_anchor –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é –ø–µ—Ä–µ—Å—á—ë—Ç —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π")
        return
    if not table_exists(conn, "batch_anchor_diff", "public"):
        print("‚ÑπÔ∏è batch_anchor_diff –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é –ø–µ—Ä–µ—Å—á—ë—Ç —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π")
        return
    if not table_exists(conn, "batch_daily_lifecycle", "public"):
        print("‚ÑπÔ∏è batch_daily_lifecycle –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é –ø–µ—Ä–µ—Å—á—ë—Ç —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π")
        return

    print("üß© –ü–µ—Ä–µ—Å—á—ë—Ç —Ç–∞–±–ª–∏—Ü—ã —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π (detail) –ø–æ —è–∫–æ—Ä—è–º...")

    with conn.cursor() as cur:
        # 0) –æ—á–∏—â–∞–µ–º detail
        try:
            cur.execute("TRUNCATE TABLE public.batch_anchor_diff;")
            conn.commit()
        except Exception as e:
            conn.rollback()
            raise RuntimeError(f"–ù–µ —Å–º–æ–≥ TRUNCATE public.batch_anchor_diff: {e}")

        # 1) –µ—Å–ª–∏ —è–∫–æ—Ä–µ–π –Ω–µ—Ç ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º –ø—É—Å—Ç–æ–π
        cur.execute("SELECT COUNT(*) FROM public.batch_manual_anchor;")
        anchors_cnt = int(cur.fetchone()[0])
        if anchors_cnt == 0:
            conn.commit()
            print("‚úÖ –Ø–∫–æ—Ä–µ–π –Ω–µ—Ç ‚Äî batch_anchor_diff –æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –ø—É—Å—Ç–æ–π")
            return

        # 2) —Å–æ–±–∏—Ä–∞–µ–º detail –ø–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–µ:
        #    —Ç–æ–≤–∞—Ä—ã –∏–∑ –∏–Ω–≤–µ–Ω—Ç—ã + –≤—Å–µ –ø–∞—Ä—Ç–∏–∏ –ø–æ –Ω–∏–º –Ω–∞ anchor_day
        sql = """
        WITH inv_scope AS (
          SELECT DISTINCT
            department,
            anchor_day,
            product_num
          FROM public.batch_manual_anchor
        ),
        plan_scope AS (
          SELECT
            l.department,
            l.product_num,
            l.product_name,
            l.snapshot_day AS anchor_day,
            l.production_day,
            l.qty_closing  AS plan_qty
          FROM public.batch_daily_lifecycle l
          JOIN inv_scope s
            ON s.department = l.department
           AND s.anchor_day = l.snapshot_day
           AND s.product_num = l.product_num
        ),
        fact_scope AS (
          SELECT
            a.department,
            a.product_num,
            a.product_name,
            a.anchor_day,
            a.production_day,
            a.qty_fact AS fact_qty
          FROM public.batch_manual_anchor a
        ),
        x AS (
          SELECT
            COALESCE(f.department, p.department) AS department,
            COALESCE(f.product_num, p.product_num) AS product_num,
            COALESCE(f.product_name, p.product_name, ref.product_name) AS product_name,
            COALESCE(f.anchor_day, p.anchor_day) AS anchor_day,
            COALESCE(f.production_day, p.production_day) AS production_day,

            COALESCE(p.plan_qty, 0)::numeric AS plan_qty,
            COALESCE(f.fact_qty, 0)::numeric AS fact_qty,
            (COALESCE(f.fact_qty, 0) - COALESCE(p.plan_qty, 0))::numeric AS diff_qty
          FROM plan_scope p
          FULL OUTER JOIN fact_scope f
            ON f.department = p.department
           AND f.product_num = p.product_num
           AND f.anchor_day = p.anchor_day
           AND f.production_day = p.production_day
          LEFT JOIN public.prep_items_ref ref
            ON public.canon_product_num(ref.product_num) = public.canon_product_num(COALESCE(f.product_num, p.product_num))
        )
        INSERT INTO public.batch_anchor_diff (
          department,
          product_num,
          product_name,
          anchor_day,
          production_day,
          plan_qty,
          fact_qty,
          diff_qty,
          created_at,
          updated_at
        )
        SELECT
          department,
          product_num,
          product_name,
          anchor_day,
          production_day,
          plan_qty,
          fact_qty,
          diff_qty,
          now(),
          now()
        FROM x
        WHERE plan_qty <> 0 OR fact_qty <> 0;
        """

        try:
            cur.execute(sql)
            conn.commit()
            print("‚úÖ batch_anchor_diff –ø–µ—Ä–µ—Å–æ–±—Ä–∞–Ω–∞ (—Ç–æ–≤–∞—Ä—ã –∏–∑ –∏–Ω–≤–µ–Ω—Ç—ã + –≤—Å–µ –ø–∞—Ä—Ç–∏–∏ –ø–æ –Ω–∏–º)")
        except Exception as e:
            conn.rollback()
            raise RuntimeError(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ—Å—á—ë—Ç–∞ batch_anchor_diff: {e}")


def main():
    date_from, date_to = get_period()
    print(f"üöÄ ETL STOCK TX: {date_from} ‚Äì {date_to}")
    print(f"üåê IIKO_BASE_URL: {IIKO_BASE_URL}")

    token = get_token()
    try:
        rows = fetch_stock_tx(token, date_from, date_to)

        conn = get_pg_connection()
        try:
            n = upsert_stock_tx(conn, rows)
            print(f"‚úÖ –í stock_tx_iiko upsert'–Ω—É—Ç–æ —Å—Ç—Ä–æ–∫: {n}")
            print_db_sample(conn, date_from, date_to)

            # ‚úÖ 1) –û–±–Ω–æ–≤–ª—è–µ–º –≤–∏—Ç—Ä–∏–Ω—É –¥–ª—è DataLens
            refresh_datalens_tail(conn, date_from, date_to)

            # ‚úÖ 2) –°–†–ê–ó–£ –ü–û–°–õ–ï –≤–∏—Ç—Ä–∏–Ω—ã –ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π –ø–æ —è–∫–æ—Ä—è–º
            #    (–ø–æ–ª–Ω–∞—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞ –∫–∞–∂–¥—ã–π –∑–∞–ø—É—Å–∫ -> —É–¥–∞–ª–∏–ª —è–∫–æ—Ä—è -> —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –∏—Å—á–µ–∑–ª–∏)
            refresh_anchor_discrepancies(conn)

        finally:
            conn.close()
            print("üîå –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Postgres –∑–∞–∫—Ä—ã—Ç–æ")
    finally:
        logout(token)


if __name__ == "__main__":
    main()
